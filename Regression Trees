# Orion Darley

# Clear global environment
rm(list=ls())

# Load packages
library(class)
library(corrplot)
library(dplyr)
library(forecast)
library(RCurl)
library(ISLR)
library(ggplot2)
library(leaps)
library(gridExtra)
library(MASS)
library(tree)
library(rpart.plot)
library(randomForest)

##########################################
#Exercise 8 of Section 8.4 of ISLR.
##########################################

#8. In the lab, a classification tree was applied to the Carseats data set after
#converting Sales into a qualitative response variable. Now we will
#seek to predict Sales using regression trees and related approaches,
#treating the response as a quantitative variable.

set.seed(9999)

#(a) Split the data set into a training set and a test set.

train = sample(1:nrow(Carseats), nrow(Carseats) / 2)
carseats.test = Carseats[-train, "Sales"]

#(b) Fit a regression tree to the training set. Plot the tree, and interpret
#the results. What test MSE do you obtain?

summary(Carseats)
tree.carseats = tree(Sales ~ ., Carseats, subset = train)
summary(tree.carseats) #Residual mean deviance = MSE
#MSE = 1.766, too many nodes; req pruning

plot(tree.carseats)
text(tree.carseats, pretty = 0)

#Unpruned Tree from CV results, using the unpruned tree 
#to make predictions against the test set
yhat = predict(tree.carseats, newdata = Carseats[-train,])
plot(yhat,carseats.test); abline(0,1)
mean((yhat - carseats.test)^2) #MSE ??? 5.8; sqrt(5.884527)

################################################
#Rpart.plot
rpart.carseats = rpart(Sales ~ ., Carseats, subset = train)
summary(rpart.carseats) #Residual mean deviance = MSE
#MSE = 2.015, too many nodes; req pruning
rpart.plot(rpart.carseats)
################################################

#(c) Use cross-validation in order to determine the optimal level of
#tree complexity. Does pruning the tree improve the test MSE?

cv.carseats = cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = 'b', main = paste("Tree Pruning on a Cross-Validation dataset"), 
     col = "steelblue4", lwd = 2, xlab = "Size", ylab = "Dev")

prun.carseats.4 = prune.tree(tree.carseats, best = 4);summary(prun.carseats.4)
plot(prun.carseats.4); text(prun.carseats.4, pretty = 0)
prun.carseats.8 = prune.tree(tree.carseats, best = 8);summary(prun.carseats.8)
plot(prun.carseats.8); text(prun.carseats.8, pretty = 0)
prun.carseats.9 = prune.tree(tree.carseats, best = 9); summary(prun.carseats.9)
plot(prun.carseats.9); text(prun.carseats.9, pretty = 0)

#(d) Use the bagging approach in order to analyze this data. What
#test MSE do you obtain? Use the importance() function to determine
#which variables are most important.

bag.carseats = randomForest(Sales~., data = Carseats, subset = train, mtry=5, ntree=25, importance=T)
yhat.bag = predict(bag.carseats, Carseats[-train,])
importance(bag.carseats)
varImpPlot(bag.carseats, col = "blue",pch=16,cex=1) #try ,type = 2
mean((yhat.bag - carseats.test)^2) #3.202519

#(e) Use random forests to analyze this data. What test MSE do you
#obtain? Use the importance() function to determine which variables
#aremost important. Describe the effect of m, the number of
#variables considered at each split, on the error rate
#obtained.

rf.carseats = randomForest(Sales~., data = Carseats, subset = train, mtry=5, ntree=25, importance=T)
yhat.rf = predict(rf.carseats, Carseats[-train,])
importance(rf.carseats)
varImpPlot(rf.carseats, col = "blue",pch=16,cex=1)
mean((yhat.rf - carseats.test)^2) #3.158899


##########################################
#Exercise 9 of Section 8.4 of ISLR.
##########################################

#9. This problem involves the OJ data set which is part of the ISLR
#package.

set.seed(9999)
data(OJ)
summary(OJ)

#(a) Create a training set containing a random sample of 800 observations,
#and a test set containing the remaining observations.

train = sample(dim(OJ)[1], 800)
oj.train = OJ[train,]
oj.test = OJ[-train,]

#(b) Fit a tree to the training data, with Purchase as the response
#and the other variables as predictors. Use the summary() function
#to produce summary statistics about the tree, and describe the
#results obtained. What is the training error rate? How many
#terminal nodes does the tree have?

tree.oj = tree(Purchase ~ ., data = oj.train)
summary(tree.oj)

#(c) Type in the name of the tree object in order to get a detailed
#text output. Pick one of the terminal nodes, and interpret the
#information displayed.

tree.oj

#(d) Create a plot of the tree, and interpret the results.

plot(tree.oj)
text(tree.oj, pretty = 0)

#(e) Predict the response on the test data, and produce a confusion
#matrix comparing the test labels to the predicted test labels.
#What is the test error rate?

tree.pred.oj = predict(tree.oj, oj.test, type= "class")
table(tree.pred.oj, oj.test$Purchase)
tree.error = 1 - ((120 + 92) / 270); tree.error #error rate 

#(f) Apply the cv.tree() function to the training set in order to
#determine the optimal tree size.

cv.oj = cv.tree(tree.oj); cv.oj

#(g) Produce a plot with tree size on the x-axis and cross-validated
#classification error rate on the y-axis.

plot(cv.oj$size, cv.oj$dev, type = 'b', main = paste("Tree Pruning on the OJ Cross-Validation Dataset"), 
     col = "steelblue4", lwd = 2, xlab = "Size", ylab = "Dev")

#(h) Which tree size corresponds to the lowest cross-validated classification
#error rate?

pruned.oj = prune.tree(tree.oj, best=6); summary(pruned.oj)

#(i) Produce a pruned tree corresponding to the optimal tree size
#obtained using cross-validation. If cross-validation does not lead
#to selection of a pruned tree, then create a pruned tree with five
#terminal nodes.

#(j) Compare the training error rates between the pruned and unpruned
#trees. Which is higher?

#(k) Compare the test error rates between the pruned and unpruned
#trees. Which is higher?

unpruned.pred.oj = predict(tree.oj, oj.test, type = "class")
unpruned = sum(oj.test$Purchase != unpruned.pred.oj)
unpruned / length(unpruned.pred.oj)

pruned.pred.oj = predict(pruned.oj, oj.test, type = "class")
pruned = sum(oj.test$Purchase != pruned.pred.oj)
pruned / length(unpruned.pred.oj)
